[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog! This is Zicun Cong. I’m a staff data scientist at a cyber security company, where I spend my days using machine learning to protect businesses and individuals from digital threats. Prior to that I spent several years pursuasing my PhD in trustworthy machine learning with a specific focus on model interpretability and fairness. I have broad interest in end to end data science, including cloud computing, database, data mining and machine learning. Outside of work, I’m passionate about sharing my knowledge and insights with others who are interested in the intersection of data science and cyber security."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zicun Cong",
    "section": "",
    "text": "Hi there!\n\nI’m a Staff Software Engineer at Fortinet, in charge of developing Machine Learning models for cybersecurity threats detection and prevention. I am passionate about developing Data Mining and Machine Learning algorithms that can extract insights from very large scale log data, graphs and text corpus, such as Frequent Pattern Mining and Graph Neural Networks. Prior to Fortinet, I worked at Datastory as a software engineer, working on analyzing social network data.\n\n\nI received my Ph.D. in Computing Science from Simon Fraser University in 2022, advised by Professor Jian Pei. Prior to that, I obtained my Master degree in Computing Science from Simon Fraser University and Bachelor degree from the School of Software Engineering, Sun Yat-sen University.\n\n\nPlease visit here for my full CV.\n\n\n\nResearch Interest\nI am interested in Cloud Computing, Trustworhty Data Analytics, and Data Pricing. My current focuses include:\n\nInterpretation on deep neural networks and statistical hypothesis\nFairness on graph neural networks\nEfficient, scalable, and interpretable data pricing models\nComputation infrastructure for ML and MLOp\n\n\n\nPublications\n\nZicun Cong, Baoxu Shi, Shan Li, Jaewon Yang, Jian Pei, Qi He. “FairSample: Training Fair and Accurate Graph Neural Networks Efficiently.” Under review\nJay Xu, Jian Pei, Zicun Cong. “Finding Multidimensional Simpson’s Paradox.” SIGKDD Explorations  [paper]\nXuan Luo, Jian Pei, Zicun Cong, Cheng Xu. “On Shapley Value in Data Assemblage Under Independent Utility.” Proc. VLDB Endow. 15, 11 (2022), 2761–2773. [paper]\nZicun Cong, Xuan Luo, Jian Pei, Feida Zhu, Yong Zhang. “Data Pricing in Machine Learning Pipelines.” Knowledge and Information Systems (KAIS), 2022. [paper]\nZicun Cong, Lingyang Chu, Yu Yang, Jian Pei.”Comprehensible counterfactual explanation on Kolmogorov-Smirnov test.” Proc. VLDB Endow. 14, 9 (May 2021), 1583–1596.  [paper]\nZicun Cong, Lingyang Chu, Lanjun Wang, Xia Hu, and Jian Pei. “Exact and Consistent Interpretation of Piecewise Linear Models Hidden behind APIs: A Closed Form Solution.” In 2020 IEEE 36th International Conference on Data Engineering (ICDE), pp. 613-624. IEEE, 2020. [paper]\n\n\n\nTalk\n\nJian Pei, Feida Zhu, Zicun Cong, Xuan Luo, Huiwen Liu, Xin Mu.. “Data Pricing and Data Asset Governance in the AI Era.” In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining (KDD ’21). Association for Computing Machinery, New York, NY, USA, 4058–4059. [tutorial webpage]"
  },
  {
    "objectID": "posts/fed_trans/index.html",
    "href": "posts/fed_trans/index.html",
    "title": "Federated Malicious URL Detection with Flower and Transformers",
    "section": "",
    "text": "Cybersecurity is becoming increasingly important in today’s digital landscape, and malicious URLs are one of the most common ways for attackers to compromise user systems. Traditionally, to buil a malicious URL detection model, a large amount of user data needs to be collected and stored at a centarlized server, which poses a significant privacy risk, especially for agencies with highly sensitive information, like banks. Despite the fact that individual users can create their own URL classifiers using their own data, the performance of these classifiers is often unsatisfactory due to the limited amount of data available to each user. Federated learning is a technique that enables users to collaboratively create a classifier that utilizes their large datasets while also prevsering data privacy.\nIn this project, we demonstrate the use of federated learning and transformers for malicious URL detection using the popuarly used Flower and Hugging Face Transformers libraries."
  },
  {
    "objectID": "posts/fed_trans/index.html#client-function",
    "href": "posts/fed_trans/index.html#client-function",
    "title": "Federated Malicious URL Detection with Flower and Transformers",
    "section": "Client Function",
    "text": "Client Function\nThe Client interface serves as the primary means of communication between the central server and clients. To reduce computation and communication cost, only a subset of clients are selected for model training during each epoch (also known as a communication round). The central server sends the parameters of the global model and the training instructions to the selected clients over the network. The selected clients then perform model training and evaluation using their local data and send the updated model parameters back to the central server.\nclass MalURLClient(fl.client.NumPyClient):\n    def __init__(self, cid, net, trainloader, valloader):\n        \"\"\"\n        Parameters\n        ----------\n        cid: \n            Client ID\n        net: \n            Model object\n        trainloader: \n            Dataloader for local training dataset\n        valloader: \n            Dataloader for validation dataset\n        \"\"\"\n        self.net = net\n        self.trainloader = trainloader\n        self.testloader = valloader\n        self.cid = cid   \n    # Other necessary functions, which will be introduced below\nImplementing Client interface typically involves defining the following methods (although set_parameters is optional): get_parameters, set_parameters, fit, and evaluate.\nHere, we have the implementation details of the four functions\nget_parameters: return the model weight as a list of NumPy ndarrays\ndef get_parameters(self, config):\n    \"\"\"\n    Parameters\n    ----------\n    config:\n        Configuration parameters requested by the server.\n        This can be used to tell the client which parameters\n        are needed along with some Scalar attributes.\n    Returns\n    -------\n    parameters: \n        The local model parameters as a list of NumPy ndarrays.\n    \"\"\"\n    return [val.cpu().numpy() for _, val in self.net.state_dict().items()]\nset_parameters: update the local model weights with the parameters received from the server\ndef set_parameters(self, net, parameters):\n    \"\"\"\n    Parameters\n    ----------\n    net: \n        The model to be trained\n    parameters: \n        The model paremters received from the central server\n    \"\"\"\n    params_dict = zip(net.state_dict().keys(), parameters)\n    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n    net.load_state_dict(state_dict, strict=True)\n    return net\nfit: performs four operations 1. set the local model weights 2. train the local model 3. receive the updated local model weights\ndef fit(self, parameters, config):\n    \"\"\"\n    Parameters\n    ----------\n    parameters: \n        The model parameters received from the central server\n    config: \n        Configuration parameters which allow the\n        server to influence training on the client. It can be used to communicate arbitrary values from the server to the client, for example, to set the number of (local) training epochs.\n    Returns\n    -------\n    parameters: \n        The locally updated model parameters.\n    num_examples:\n        The number of examples used for training.\n    metrics:\n        A dictionary mapping arbitrary string keys to values of type\n        bool, bytes, float, int, or str. It can be used to communicate\n        arbitrary values back to the server.\n    \"\"\"\n    set_parameters(net, parameters)\n    print(\"Training Started...\")\n    train_client(self.net, self.trainloader, epochs=1)\n    print(\"Training Finished.\")\n    return self.get_parameters(config), len(self.trainloader), {}\nevaluate: test the local model\ndef evaluate(self, parameters, config):\n    \"\"\"\n    Evaluate the provided parameters using the locally held dataset.\n    Parameters\n    ----------\n    parameters :\n        The current (global) model parameters.\n    config : \n        Same as the config in fit function\n    Returns\n    -------\n    loss : float\n        The evaluation loss of the model on the local dataset.\n    num_examples : int\n        The number of examples used for evaluation.\n    metrics : Dict[str, Scalar]\n        A dictionary mapping arbitrary string keys to values of\n        type bool, bytes, float, int, or str. It can be used to\n        communicate arbitrary values back to the server.\n    \"\"\"\n    self.net = set_parameters(net, parameters)\n    # test function is defined in the utility file.\n    valid_loss, valid_accuracy, valid_f1 = test(self.net, self.valloader)\n    metrics = {\n        \"valid_accuracy\": float(valid_accuracy), \n        \"valid_loss\": float(valid_loss),\n        'valid_f1': float(valid_f1),\n    }\n    return float(valid_loss), len(self.testloader), rsmetricst\nAs in the first post, we’re going to start by writing a helper file named flower_helpers.py where most of our functions will be located. Starting with imports:\nAll the libraries needed are here: Flower (flwr), Torch + Torchivision, Numpy, and Opacus. Some others are for typing concerns. You can notice we imported FedAvg from Flower, which is the strategy used by the library to define how weights are updated in the federated process. We need to create our strategy to adapt to the DP case. From Opacus only two things are imported: the PrivacyEngine and the sampler. The engine will let us attach it to any torch optimizer to perform the DP-SGD steps on it. As for the sampler, we will see about it in a while. The next step is defining our device for the model:"
  },
  {
    "objectID": "posts/fed_trans/index.html#client",
    "href": "posts/fed_trans/index.html#client",
    "title": "Federated Malicious URL Detection with Flower and Transformers",
    "section": "Client",
    "text": "Client\nThe Client interface serves as the primary means of communication between the central server and clients. To reduce computation and communication cost, only a subset of clients are selected for model training during each epoch (also known as a communication round). The central server sends the parameters of the global model and the training instructions to the selected clients over the network. The selected clients then perform model training and evaluation using their local data and send the updated model parameters back to the central server.\nclass MalURLClient(fl.client.NumPyClient):\n    def __init__(self, cid, net, trainloader, valloader):\n        \"\"\"\n        Parameters\n        ----------\n        cid: \n            Client ID.\n        net: \n            Model object.\n        trainloader: \n            Dataloader for local training dataset.\n        valloader: \n            Dataloader for validation dataset.\n        \"\"\"\n        self.net = net\n        self.trainloader = trainloader\n        self.testloader = valloader\n        self.cid = cid   \n    # Other necessary functions, which will be introduced below\nImplementing Client interface typically involves defining the following methods (although set_parameters is optional): get_parameters, set_parameters, fit, and evaluate. Here, we have the implementation details of the four functions\nget_parameters: return the model weight as a list of NumPy ndarrays\ndef get_parameters(self, config):\n    \"\"\"\n    Parameters\n    ----------\n    config:\n        Configuration parameters requested by the server.\n        This can be used to tell the client which parameters\n        are needed along with some Scalar attributes.\n    Returns\n    -------\n    parameters: \n        The local model parameters as a list of NumPy ndarrays.\n    \"\"\"\n    return [val.cpu().numpy() for _, val in self.net.state_dict().items()]\nset_parameters: update the local model weights with the parameters received from the server\ndef set_parameters(self, net, parameters):\n    \"\"\"\n    Parameters\n    ----------\n    net: \n        The model to be trained.\n    parameters: \n        The model paremters received from the central server.\n    \"\"\"\n    params_dict = zip(net.state_dict().keys(), parameters)\n    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n    net.load_state_dict(state_dict, strict=True)\n    return net\nfit: performs four operations 1. set the local model weights 2. train the local model 3. receive the updated local model weights\ndef fit(self, parameters, config):\n    \"\"\"\n    Parameters\n    ----------\n    parameters: \n        The model parameters received from the central server.\n    config: \n        Configuration parameters which allow the\n        server to influence training on the client. It can be used to communicate arbitrary values from the server to the client, for example, to set the number of (local) training epochs.\n    Returns\n    -------\n    parameters: \n        The locally updated model parameters.\n    num_examples:\n        The number of examples used for training.\n    metrics:\n        A dictionary mapping arbitrary string keys to values of type\n        bool, bytes, float, int, or str. It can be used to communicate\n        arbitrary values back to the server.\n    \"\"\"\n    set_parameters(net, parameters)\n    print(\"Training Started...\")\n    # train_client function train the local model using the client' local dataset.\n    # train_client function is defined in the utility file\n    train_client(self.net, self.trainloader, epochs=1)\n    print(\"Training Finished.\")\n    return self.get_parameters(config), len(self.trainloader), {}\nevaluate: evaluate the global model on the client’s local dataset\ndef evaluate(self, parameters, config):\n    \"\"\"\n    Evaluate the provided parameters using the locally held dataset.\n    Parameters\n    ----------\n    parameters :\n        The current (global) model parameters.\n    config : \n        Same as the config in fit function.\n    Returns\n    -------\n    loss : \n        The evaluation loss of the model on the local dataset.\n    num_examples : \n        The number of examples used for evaluation.\n    metrics : \n        A dictionary mapping arbitrary string keys to values of\n        type bool, bytes, float, int, or str. It can be used to\n        communicate arbitrary values back to the server.\n    \"\"\"\n    self.net = set_parameters(net, parameters)\n    # test function is defined in the utility file.\n    valid_loss, valid_accuracy, valid_f1 = test(self.net, self.valloader)\n    metrics = {\n        \"valid_accuracy\": float(valid_accuracy), \n        \"valid_loss\": float(valid_loss),\n        'valid_f1': float(valid_f1),\n    }\n    return float(valid_loss), len(self.testloader), rsmetricst\nNext, let’s create an instance of our client class, and run it using the following line:\nfl.client.start_numpy_client(server_address=\"[::]:8080\", client=CifarClient())\nThe string [::]:8080 specifies which server the client should connect to. The string [::] means that the server and client are running on the same machine. If we were to run a truly federated learning with the server and clients on different machines, we would need to update the server_address to point the client to the correct server."
  },
  {
    "objectID": "posts/fed_trans/index.html#server",
    "href": "posts/fed_trans/index.html#server",
    "title": "Federated Malicious URL Detection with Flower and Transformers",
    "section": "Server",
    "text": "Server\nNow that we have a way to instantiate clients, we need to create our server in order to aggregate the results. Using Flower, this can be done very easily by first choosing a strategy. A stratey is about how the central server update the global model using the model updates aggregated from the clients. In this project, we adopt the celebrated FedAvg, which will define the global weights as the average of all the clients’ weights at each round.\nIt is very important to decide how to evaluate the learned model. In federated learning, two types of evaluations are commonly used, namely, centarlize evaluation and federated evaluation. The two evaluation methods can be used at the same time.\nCentralized Evaluation (or server-side evaluation): it works similarly to evaluation in centralized machine learning. If there’s a server-side dataset available, we can use it to evaluate the newly aggregated model after each round of training.\nFederated evaluation (or client-side evaluation): the central server sends the newly aggregated model to a set of selected clients. Each client then evaluates the model on its local dataset and sends the evaluation metrics back to the central server. The central server aggregated the received the metrics, such as by taking the average, as the evaluation metric.\nFederated evaluation is more powerful than centralized evaluation because it allows for evaluation over a larger set of data, which often leads to more realistic evaluation results. However, federated evaluation is more complex and requires caution. Since in each epoch, a subset of clients are randomly selected for model evaluation, the evaluation clients can change over consecutive rounds of learning, leading to unstable evaluation results even if the model remains unchanged.\nWe can mitigate the issue by selecting all clients for evaluation for stable evaluation, however, this results in large communication and computation cost, especially there are hundreds of or thousands of clients participating in the federated learning.\nNext, let’s implement our server using the FedAvg strategy provided by Flower.\nevaluate: the centralized evaluation function, which will be called by the server after every epoch of federated learning.\ndef get_evaluate_fn(net, testloader):\n    \"\"\"\n    Return an evaluation function for centralized evaluation.\n    Parameters\n    ----------\n    net:\n        Model to be evaluated.\n    testloader:\n        Dataset loader.\n    \"\"\"\n    def evaluate(server_round, parameters, config):\n        \"\"\"\n        Parameters\n        ----------\n        server_round:\n            The current epoch of federated learning.\n        parameters:\n            The current (global) model parameters.\n        config : \n            Same as the config in fit.\n        Returns\n        -------\n        loss : float\n            The evaluation loss of the model on the local dataset.\n        \"\"\"\n        set_parameters(net, parameters)  # 'net' is the global model .Update model with the latest parameters\n        loss, accuracy, f1 = test(net, testloader)\n        return loss, {\"accuracy\": accuracy, 'f1': f1}\n\n    return evaluate\nweighted_average: metric aggregation used by federated evaluation.\ndef weighted_average(metrics: List[Tuple[int, Metrics]]) -&gt; Metrics:\n    \"\"\"\n    Multiply accuracy of each client by number of examples used.\n    Aggregate and return custom metric (weighted average).\n\n    Parameters\n    ----------\n    metrics:\n        The list of local evaluation metrics sent by clients.\n        metrics[idx] is the evaluation sent by the `idx` evaluation client.\n        metrics[idx][0] is the number of records of the corresponding client.\n        metrics[idx][1] is the evaluation metrics of the corresponding clients.\n\n    Returns\n    -------\n    weight_metrics :\n        The weighted average of the federated evaluation.\n    \"\"\"\n    weight_metrics = {}\n    for metric_name in metrics[0][1].keys():\n      for num_examples, m in metrics:\n        metric = [num_examples * m[metric_name] for num_examples, m in metrics]\n        examples = [num_examples for num_examples, _ in metrics]\n        weight_metrics[metric_name] = sum(metric) / sum(examples)\n    \n    return weight_metrics\nNow, we are ready to create a FedAvg stretegy using the above defined federated and centralized evaluation functions.\nstrategy = fl.server.strategy.FedAvg(\n    fraction_fit = 2/NUM_CLIENTS, # Sample 2 available clients for training in each epoch\n    evaluate_metrics_aggregation_fn = weighted_average, # Use weighted average function to aggregate the local evaluation metrics of clients. \n    fraction_evaluate = 2/NUM_CLIENTS, # Sample 2 available clients for model evaluation\n    evaluate_fn=get_evaluate_fn(net, testloader)  # Pass the evaluation function\n)"
  },
  {
    "objectID": "posts/fed_trans/index.html#utility-functions",
    "href": "posts/fed_trans/index.html#utility-functions",
    "title": "Federated Malicious URL Detection with Flower and Transformers",
    "section": "Utility Functions",
    "text": "Utility Functions\nGreat work! We’re halfway through our journey. In this section, we’ll implement some helper functions, including a data split function and model training and evaluation functions. Let’s get started!\nThe distributions of the local datasets held by clients can have a significant impact on the performance of the collaboratively trained model in federated learning. This is because federated learning relies on stochastic gradient descent (SGD), and the independent and identically distributed (IID) sampling of the training data is important to ensure that the stochastic gradient is an unbiased estimate of the full gradient (Zhao et al. 2018).\nTo investigate the impact of local data distribution on the performance of the global model, we conducted experiments using two different methods to generate clients’ local data, that is, IID data generation and non-IID data generation.\nprepare_train_test_even: partition the dataset into the local training data of clients, and a server side testing data.\ndef _train_test_split(loca_data):\n    \"\"\"\n    Split the given dataset to training and testing datasets\n    Parameters\n    ----------\n    loca_data:\n        The local dataset of a client, which will be split into training and validation datasets.\n\n    Returns\n    -------\n    data_dict:\n        The training and validation datasets of a client\n    \"\"\"\n    train_val_client_split = loca_data.train_test_split(test_size=0.2, seed=42)  # 80% local training data, 20% local validation data\n    data_dict = DatasetDict({\n                'train': train_val_client_split['train'],\n                'validation': train_val_client_split['test'],\n                })\n    return data_dict\n\ndef prepare_train_test_iid(raw_datasets):\n    \"\"\"\n    Generate IID datasets for clients.\n\n    Parameters\n    ----------\n    raw_datasets:\n        The entire dataset will be split into local datases for each client.\n    \n    Returns\n    -------\n    client_datasets:\n        The list of client local datasets will consist of DataDict objects, with each object containing the training and validation datasets for a single client. \n    server_test_dataset:\n        The dataset used by the central server for central evaluation.\n    \"\"\"\n    train_test_split = raw_datasets.train_test_split(test_size=0.2, seed=42)\n    client_dataset = train_test_split['train']\n    server_test_dataset = train_test_split['test']\n    partition_size = len(global_train_dataset) // NUM_CLIENTS # `NUM_CLIENTS` is the total number of clients in the federated learning process. `partition_size` is the number of records in each client's local data.\n\n    client_datasets = []\n    for _ in range(NUM_CLIENTS):\n        client_split = client_dataset.train_test_split(train_size=partition_size)\n        client_dataset = client_split['test'] # The remaining data will be divided among the other clients.\n        client_datasets.append(_train_test_split(client_split['train']))\n    return client_datasets, server_test_dataset\nprepare_train_test_noniid: generate label-based non-IID distributed local datasets for clients. Assume we are working on a multi-class classification problem. A client is only assigned records from two specified categories in the dataset. For example, consider that we are working on a malicious URLs detection problem, where there are four categories, namely, benign, malicious, phishing, and defacement URLs. The first client will only have records from benign and phishing categories, the second client will only have records from benign and malicous categories, and the third client will only have records from benign and defacement categories.\ndef prepare_train_test_noniid(raw_datasets):\n    \"\"\"\n    Generate non-IID datasets for clients.\n\n    Parameters and returns are the same as those `prepare_train_test_iid`\n    \"\"\"\n    train_test_split = raw_datasets.train_test_split(test_size=0.2, seed=42)\n    clients_dataset, server_test_dataset = train_test_split['train'], train_test_split['test']\n    # label_id 0: benign\n    # label_id 1: malicious\n    # label_id 2: phishing\n    # label_id 3: defacement\n    whole_benign = clients_dataset.filter(lambda x: x['labels'] == 0)\n    benign_size_per_client = len(whole_bening) // NUM_CLIENTS\n\n    client_datasets = []\n    # Class 0 is benign\n    for cid in range(NUM_CLIENTS):\n        abnormal_urls = clients_dataset.filter(lambda x: x['labels'] == (cid + 1)) \n        client_split = whole_benign.train_test_split(train_size=benign_size_per_client, seed=42)\n        local_benign, whole_benign = client_split['train'], client_split['test']\n\n        local_dataset = concatenate_datasets([local_benign, abnormal_urls])\n        client_datasets.append(_train_test_split(local_dataset))\n\n    return client_datasets, server_test_dataset\nFor this project, we constructed our classifier using the pre-trained BERT model. To prepare the input data for the model, we used the tokenize function of BERT to tokenize each URL into a sequence of tokens compatible with the BERT model. We iterate the datasets generated from the above functions, and tokenize the datasets. The tokenizer is also provided by the HuggingFace Transformers library. In particular, tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\ndef tokenize_function(record):\n    return tokenizer(record[\"url\"], truncation=True, padding=True)\n\nfor client_dataset in client_datasets:\n    tokenized_datasets = (client_dataset\n                        .map(tokenize_function, batched=True)\n                        .remove_columns(\"url\"))\n    \n    # The main purpose of DataCollatorWithPadding is to dynamically pad input sequences in a batch with the padding token to match the longest sequence in that batch. \n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n    \n    # Wrap the tokenized dataset as DataLoader\n    trainloader = DataLoader(\n        tokenized_datasets[\"train\"],\n        shuffle=True,\n        batch_size=BATCH_SIZE,\n        collate_fn=data_collator,\n    )\n    # Wrap test and validation dataset as DataLoader, ....\nAfter prepraing our datasets, next step is to define our model. For this project, we build our classifier based on the pre-trained BERT model, which can be easily implemented using the API provided by HuggingFace Transformers.\nThe created model instance net consists of a pre-trained distilled BERT model and a linear layer for sequence label prediction.\ndef init_model(num_labels, fine_tune=True):\n    \"\"\"\n    Initialize a BERT based sequence classifier.\n\n    Parameters\n    ----------\n    fine_tune:\n        If we want to fine tune the parameters of the pre-trained BERT model.\n    \"\"\"\n    CHECKPOINT = \"distilbert-base-uncased\" # Model name\n    net = AutoModelForSequenceClassification.from_pretrained(CHECKPOINT, num_labels=num_labels).to(DEVICE)\n\n    if fine_tune == False:\n        for name, param in net.named_parameters():\n            if name.startswith(\"bert\"): # choose whatever you like here\n                param.requires_grad = False\n\n    net.train()\n    return net\nWe are almost done with our implementation. The final step is to implement the training function to train our model.\ndef train(net, trainloader, epochs, testloader):\n    \"\"\"\n    The training function is used by clients to train their local model during the federated learning process.\n\n    Parameters\n    ----------\n    net:\n        The local model to be trained.\n    trainloader:\n        The DataLoader of training data.\n    epochs:\n        The number of epochs for client training during the federated learning process.\n    testloader:\n        The DataLoader of test data.\n    \"\"\"\n    optimizer = AdamW(net.parameters(), lr=5e-5)\n\n    for _ in tqdm.tqdm(range(epochs), desc='epoch'):\n      net.train()\n      for batch in tqdm.tqdm(trainloader, desc='iterate data'):\n          batch = {k: v.to(DEVICE) for k, v in batch.items()}\n          outputs = net(**batch)\n          logits = outputs.get(\"logits\")\n          # Since the URL dataset is highly imbalanced with benign URLs (label 0) dominate the dataset, we use weighted loss function to tackle the problem.\n          loss_fct = nn.CrossEntropyLoss(\n                        weight=torch.tensor([1.0, 10.0, 10.0, 10.0], \n                        device=DEVICE)\n                      )\n          labels = batch.get(\"labels\")\n          loss = loss_fct(logits.view(-1, NUM_LABELS), labels.view(-1))\n          # loss = outputs.loss\n          loss.backward()\n          optimizer.step()\n          optimizer.zero_grad()\n      \n    torch.cuda.empty_cache()"
  },
  {
    "objectID": "posts/fed_trans/index.html#execution",
    "href": "posts/fed_trans/index.html#execution",
    "title": "Federated Malicious URL Detection with Flower and Transformers",
    "section": "Execution",
    "text": "Execution\nNow that we have completed the implementation of our federated learning system, it’s time to put it to the test. Now that we have completed the implementation of our federated learning system, it’s time to put it to the test. Consider we aim to simulate a federated learning system with 10 clients on a single machine. Flower provides two ways to run the system: (1) Flower launches 10 instances of FlowerClient on different machines or servers; (2) Flower offers a way to simulate a federated learning system with multiple clients on a single machine by creating FlowerClient instances only when necessary for training or evaluation. This helps prevent memory exhaustion. To enable this function, we need to implement a client_fn function that creates a FlowerClient instance on demand.\ndef get_client_fn(client_dataloaders):\n    \"\"\"\n    Return the function to create a client\n\n    Parameters\n    ----------\n    client_dataloaders:\n        Dataloader of the training data of all clients\n    \"\"\"\n    def client_fn(cid):\n        \"\"\"\n        Create a client instance\n        \n        Parameters\n        -----------\n        cid:\n            The client ID\n        \"\"\"\n        return MalURLClient(cid, net, client_dataloaders[int(cid)]['train'], client_dataloaders[int(cid)]['validation'], testloader)\n    \n    return client_fn\nIn this project, we focus on the second option of simulation. To start the simulatoin, we only a simple call the build-in function start_simulation.\n# Start simulation\nfl.simulation.start_simulation(\n    client_fn=get_client_fn(client_dataloaders),\n    num_clients=NUM_CLIENTS,\n    config=fl.server.ServerConfig(num_rounds=5),\n    strategy=strategy, # Server side strategy discussed in Section Server\n    client_resources=client_resources,\n)"
  },
  {
    "objectID": "posts/fed_trans/index.html#experiment-settings",
    "href": "posts/fed_trans/index.html#experiment-settings",
    "title": "Federated Malicious URL Detection with Flower and Transformers",
    "section": "Experiment Settings",
    "text": "Experiment Settings\nIn each experiment, we perform 10 communication rounds. By default, there are ten participants, and two participants are selected for model training per round. The selected participants execute one epoch of training on their local models."
  },
  {
    "objectID": "posts/fed_trans/index.html#results-on-the-iid-and-non-iid-settings",
    "href": "posts/fed_trans/index.html#results-on-the-iid-and-non-iid-settings",
    "title": "Federated Malicious URL Detection with Flower and Transformers",
    "section": "Results on the IID and non-IID Settings",
    "text": "Results on the IID and non-IID Settings\n\n\n\nResults\n\n\n\nReferences\n\n\nZhao, Yue, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. 2018. “Federated Learning with Non-Iid Data.” arXiv Preprint arXiv:1806.00582."
  },
  {
    "objectID": "index_bak.html",
    "href": "index_bak.html",
    "title": "Minzc",
    "section": "",
    "text": "Federated Malicious URL Detection with Flower and Transformers\n\n\n\n\n\n\n\ncode\n\n\nnlp\n\n\nfederated learning\n\n\n\n\n\n\n\n\n\n\n\nApr 21, 2023\n\n\nZicun Cong\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nApr 20, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Minzc",
    "section": "",
    "text": "Federated Malicious URL Detection with Flower and Transformers\n\n\n\n\n\n\n\ncode\n\n\nnlp\n\n\nfederated learning\n\n\n\n\n\n\n\n\n\n\n\nApr 21, 2023\n\n\nZicun Cong\n\n\n\n\n\n\nNo matching items"
  }
]