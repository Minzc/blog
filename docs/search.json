[
  {
    "objectID": "posts/building_efficient_universal_classifiers_with_natural_language_inference/index.html",
    "href": "posts/building_efficient_universal_classifiers_with_natural_language_inference/index.html",
    "title": "Building Efficient Universal Classifiers with Natural Language Inference",
    "section": "",
    "text": "Let’s face it: understanding and categorizing text can be a headache. Usually, you need to teach a computer model with lots of specific examples to get it right. But what if They could make this process easier and not so specific? That’s what this breakthrough method is all about. The paper aims to build a pre-trained language model to perform zero-shot text classification task. It uses a clever trick called Natural Language Inference (NLI) to teach models to classify text into categories without all the usual fuss."
  },
  {
    "objectID": "posts/building_efficient_universal_classifiers_with_natural_language_inference/index.html#building-a-universal-classifier-a-step-by-step-guide",
    "href": "posts/building_efficient_universal_classifiers_with_natural_language_inference/index.html#building-a-universal-classifier-a-step-by-step-guide",
    "title": "Building Efficient Universal Classifiers with Natural Language Inference",
    "section": "Building a Universal Classifier: A Step-by-Step Guide",
    "text": "Building a Universal Classifier: A Step-by-Step Guide\nGetting the Data Ready\nThey start by cleaning and organizing their data from different stheirces to make sure it’s all speaking the same language, so to speak. This step ensures everything is compatible with their model. Formulation of the input and output of the training data is shown in Figure 1.\nOptional Cleaning Up\nThere’s also a neat optional step where they clean the data further, removing any messy or confusing information. This helps their model learn better and faster, using a tool called CleanLab.\nCrafting Hypotheses\nHere’s where the NLI comes into play. They create “if this, then that” scenarios for each category. For example, if a text talks about cooking, it might belong to the “food” category. They also prepare for cases where the text doesn’t match, teaching the model to recognize when something doesn’t fit.\nTraining the Brain\nThey use a pre-trained model called DeBERTaV3 and teach it with their prepared data. Depending on the model’s size, this can take from 5 to 10 htheirs of computer time for one run, with a total of 6 to 15 days for all the training They need. Here are some of the hyperparameters They dial in for optimal performance: 1. Learning Rate: They adjust this based on the model size, setting it at 9e-6 for “large” models and 2e-5 for others. This determines how quickly the model learns from the data. 2. Gradient Accumulation Steps: This technique allows us to train with larger batches than their hardware could typically handle by spreading the computation across multiple steps. 3. Training Epochs: They run the training for 3 epochs, allowing the model to learn from the dataset three full times. 4. Warmup Ratio and Theyight Decay: These are tTheyaked to optimize training dynamics and prevent overfitting, where warmup ratio and Theyight decay are set to 0.06 and 0.01, respectively.\nOne training run on around 9000000 concatenated hypothesis-premise pairs for 3 epochs takes around 5 htheirs for DeBERTaV3-base and 10 htheirs for DeBERTaV3-large on one A100 40GB GPU.\nEvaluation\nAfter all that training, They check how well their model can classify new texts it hasn’t seen before. The model performance is shown in Figure 2. Overall, deberta-v3-zeroshot-v1.1-all-33 significantly outperforms the NLI-only model both on held-in and held-out tasks.\nThe authors have two insights. First, models trained with a mix of NLI data and non-NLI data achieve overall better zeroshot performance than the NLI-only model (+9.4% on average). Having seen different zeroshot-style hypotheses helps the model generalize to other unseen tasks and hypotheses (positive transfer). Second, there are a few cases of negative transfer. On a few datasets, the NLI-only model performs better than deberta-v3-zeroshot-v1.1-heldout, indicating that the additional task-mix can make the model over- or underpredict a few classes."
  },
  {
    "objectID": "posts/fed_trans/index.html",
    "href": "posts/fed_trans/index.html",
    "title": "Federated Malicious URL Detection with Flower and Transformers",
    "section": "",
    "text": "Cybersecurity is becoming increasingly important in today’s digital landscape, and malicious URLs are one of the most common ways for attackers to compromise user systems. Traditionally, to buil a malicious URL detection model, a large amount of user data needs to be collected and stored at a centarlized server, which poses a significant privacy risk, especially for agencies with highly sensitive information, like banks. Despite the fact that individual users can create their own URL classifiers using their own data, the performance of these classifiers is often unsatisfactory due to the limited amount of data available to each user. Federated learning is a technique that enables users to collaboratively create a classifier that utilizes their large datasets while also prevsering data privacy.\nIn this project, we demonstrate the use of federated learning and transformers for malicious URL detection using the popuarly used Flower and Hugging Face Transformers libraries."
  },
  {
    "objectID": "posts/fed_trans/index.html#client",
    "href": "posts/fed_trans/index.html#client",
    "title": "Federated Malicious URL Detection with Flower and Transformers",
    "section": "Client",
    "text": "Client\nThe Client interface serves as the primary means of communication between the central server and clients. To reduce computation and communication cost, only a subset of clients are selected for model training during each epoch (also known as a communication round). The central server sends the parameters of the global model and the training instructions to the selected clients over the network. The selected clients then perform model training and evaluation using their local data and send the updated model parameters back to the central server.\nclass MalURLClient(fl.client.NumPyClient):\n    def __init__(self, cid: str, \n                 net: torch.nn.Module, \n                 trainloader: DataLoader, \n                 valloader: DataLoader,\n                 epoch: int) -&gt; None:\n        \"\"\"\n        Initializes the class with the specified parameters.\n\n        Parameters\n        ----------\n        cid : str\n            A string representing the ID of the class.\n        net : torch.nn.Module\n            The neural network to use in the class.\n        trainloader : DataLoader\n            The data loader for the training set.\n        valloader : DataLoader\n            The data loader for the validation set.\n        epoch : int\n            The number of epochs to train for.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        self.net = net\n        self.trainloader = trainloader\n        self.valloader = valloader\n        self.cid = cid\n        self.epoch = epoch  \n    # Other necessary functions, which will be introduced below\nImplementing Client interface typically involves defining the following methods (although set_parameters is optional): get_parameters, set_parameters, fit, and evaluate. Here, we have the implementation details of the four functions\nget_parameters: return the model weight as a list of NumPy ndarrays\ndef get_parameters(self, config: dict) -&gt; List[np.ndarray]:\n    \"\"\"\n    Returns a list of the parameters of the neural network in the class.\n\n    Parameters\n    ----------\n    config : dict\n        A dictionary containing configuration parameters.\n\n    Returns\n    -------\n    List[np.ndarray]\n        A list of numpy arrays containing the parameters of the neural network.\n    \"\"\"\n    return [val.cpu().numpy() for _, val in self.net.state_dict().items()]\nset_parameters: update the local model weights with the parameters received from the server\ndef set_parameters(net: nn.Module, parameters: List[torch.Tensor]) -&gt; nn.Module:\n    \"\"\"\n    Sets the parameters of a PyTorch neural network module to the specified tensors.\n\n    Parameters\n    ----------\n    net: nn.Module\n        The neural network module to set the parameters for.\n    parameters: List[torch.Tensor]\n        The list of tensors to set the parameters of the neural network module to.\n\n    Returns\n    -------\n    nn.Module\n        The neural network module with updated parameters.\n    \"\"\"\n    params_dict = zip(net.state_dict().keys(), parameters)\n    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n    net.load_state_dict(state_dict, strict=True)\n    return net\nfit: performs four operations 1. set the local model weights 2. train the local model 3. receive the updated local model weights\ndef fit(self, parameters, config):\n    \"\"\"\n    Parameters\n    ----------\n    parameters: \n        The model parameters received from the central server.\n    config: \n        Configuration parameters which allow the\n        server to influence training on the client. It can be used to communicate arbitrary values from the server to the client,\n        for example, to set the number of (local) training epochs.\n    Returns\n    -------\n    parameters: \n        The locally updated model parameters.\n    num_examples:\n        The number of examples used for training.\n    metrics:\n        A dictionary mapping arbitrary string keys to values of type\n        bool, bytes, float, int, or str. It can be used to communicate\n        arbitrary values back to the server.\n    \"\"\"\n    set_parameters(self.net, parameters)\n    print(\"Training Started...\")\n    # train_client function train the local model using the client' local dataset.\n    # train_client function is defined in the utility file\n    train(self.net, self.trainloader, epochs=self.epoch)\n    print(\"Training Finished.\")\n    return self.get_parameters(config), len(self.trainloader), {}\nevaluate: evaluate the global model on the client’s local dataset\ndef evaluate(self, parameters, config):\n    \"\"\"\n    Evaluate the provided parameters using the locally held dataset.\n    Parameters\n    ----------\n    parameters :\n        The current (global) model parameters.\n    config : \n        Same as the config in fit function.\n    Returns\n    -------\n    loss : \n        The evaluation loss of the model on the local dataset.\n    num_examples : \n        The number of examples used for evaluation.\n    metrics : \n        A dictionary mapping arbitrary string keys to values of\n        type bool, bytes, float, int, or str. It can be used to\n        communicate arbitrary values back to the server.\n    \"\"\"\n    self.net = set_parameters(self.net, parameters)\n    # test function is defined in the utility file.\n    valid_loss, valid_accuracy, valid_f1 = test(self.net, self.valloader)\n    metrics = {\n        \"valid_accuracy\": float(valid_accuracy), \n        \"valid_loss\": float(valid_loss),\n        'valid_f1': float(valid_f1),\n    }\n    return float(valid_loss), len(self.valloader), metrics"
  },
  {
    "objectID": "posts/fed_trans/index.html#server",
    "href": "posts/fed_trans/index.html#server",
    "title": "Federated Malicious URL Detection with Flower and Transformers",
    "section": "Server",
    "text": "Server\nNow that we have a way to instantiate clients, we need to create our server in order to aggregate the results. Using Flower, this can be done very easily by first choosing a strategy. A stratey is about how the central server update the global model using the model updates aggregated from the clients. In this project, we adopt the celebrated FedAvg, which will define the global weights as the average of all the clients’ weights at each round.\nIt is very important to decide how to evaluate the learned model. In federated learning, two types of evaluations are commonly used, namely, centarlize evaluation and federated evaluation. The two evaluation methods can be used at the same time.\nCentralized Evaluation (or server-side evaluation): it works similarly to evaluation in centralized machine learning. If there’s a server-side dataset available, we can use it to evaluate the newly aggregated model after each round of training.\nFederated evaluation (or client-side evaluation): the central server sends the newly aggregated model to a set of selected clients. Each client then evaluates the model on its local dataset and sends the evaluation metrics back to the central server. The central server aggregated the received the metrics, such as by taking the average, as the evaluation metric.\nFederated evaluation is more powerful than centralized evaluation because it allows for evaluation over a larger set of data, which often leads to more realistic evaluation results. However, federated evaluation is more complex and requires caution. Since in each epoch, a subset of clients are randomly selected for model evaluation, the evaluation clients can change over consecutive rounds of learning, leading to unstable evaluation results even if the model remains unchanged.\nWe can mitigate the issue by selecting all clients for evaluation for stable evaluation, however, this results in large communication and computation cost, especially there are hundreds of or thousands of clients participating in the federated learning.\nNext, let’s implement our server using the FedAvg strategy provided by Flower.\nevaluate: the centralized evaluation function, which will be called by the server after every epoch of federated learning.\ndef get_evaluate_fn(net: torch.nn.Module, testloader: DataLoader) -&gt; Callable[[int, Any, dict], Tuple[float, dict]]:\n    \"\"\"\n    Return an evaluation function for centralized evaluation.\n\n    Parameters\n    ----------\n    net : torch.nn.Module\n        The model to be evaluated.\n    testloader : DataLoader\n        The dataset loader.\n    Returns\n    -------\n    Callable[[int, Any, dict], Tuple[float, dict]]\n        A function that evaluates the model on the given test data and returns the evaluation loss and metrics.\n    \"\"\"\n    def evaluate(server_round: int, parameters: Any, config: dict) -&gt; Tuple[float, dict]:\n        \"\"\"\n        Evaluate the model on the given test data and return the evaluation loss and metrics.\n\n        Parameters\n        ----------\n        server_round : int\n            The current epoch of federated learning.\n        parameters : Any\n            The current (global) model parameters.\n        config : dict\n            Same as the config in fit.\n\n        Returns\n        -------\n        Tuple[float, dict]\n            A tuple containing the evaluation loss and a dictionary of evaluation metrics.\n        \"\"\"\n        set_parameters(net, parameters)  # 'net' is the global model. Update model with the latest parameters\n        loss, accuracy, f1 = test(net, testloader)\n        return loss, {\"accuracy\": accuracy, 'f1': f1}\n\n    return evaluate\nweighted_average: metric aggregation used by federated evaluation.\ndef weighted_average(metrics: List[Tuple[int, Metrics]]) -&gt; Metrics:\n    \"\"\"\n    Multiply accuracy of each client by number of examples used.\n    Aggregate and return custom metric (weighted average).\n\n    Parameters\n    ----------\n    metrics: List[Tuple[int, Metrics]]\n        The list of local evaluation metrics sent by clients.\n        metrics[idx] is the evaluation sent by the `idx` evaluation client.\n        metrics[idx][0] is the number of records of the corresponding client.\n        metrics[idx][1] is the evaluation metrics of the corresponding clients.\n\n    Returns\n    -------\n    weight_metrics: Metrics\n        The weighted average of the federated evaluation.\n    \"\"\"\n    weight_metrics = {}\n    for metric_name in metrics[0][1].keys():\n      for num_examples, m in metrics:\n        metric = [num_examples * m[metric_name] for num_examples, m in metrics]\n        examples = [num_examples for num_examples, _ in metrics]\n        weight_metrics[metric_name] = sum(metric) / sum(examples)\n    \n    return weight_metrics\nNow, we are ready to create a FedAvg stretegy using the above defined federated and centralized evaluation functions.\nstrategy = fl.server.strategy.FedAvg(\n    fraction_fit = 2/NUM_CLIENTS, # Sample 2 available clients for training in each epoch\n    evaluate_metrics_aggregation_fn = weighted_average, # Use weighted average function to aggregate the local evaluation metrics of clients. \n    fraction_evaluate = 2/NUM_CLIENTS, # Sample 2 available clients for model evaluation\n    evaluate_fn=get_evaluate_fn(net, testloader)  # Pass the evaluation function\n)"
  },
  {
    "objectID": "posts/fed_trans/index.html#utility-functions",
    "href": "posts/fed_trans/index.html#utility-functions",
    "title": "Federated Malicious URL Detection with Flower and Transformers",
    "section": "Utility Functions",
    "text": "Utility Functions\nGreat work! We’re halfway through our journey. In this section, we’ll implement some helper functions, including a data split function and model training and evaluation functions. Let’s get started!\nThe distributions of the local datasets held by clients can have a significant impact on the performance of the collaboratively trained model in federated learning. This is because federated learning relies on stochastic gradient descent (SGD), and the independent and identically distributed (IID) sampling of the training data is important to ensure that the stochastic gradient is an unbiased estimate of the full gradient (Zhao et al. 2018).\nTo investigate the impact of local data distribution on the performance of the global model, we conducted experiments using two different methods to generate clients’ local data, that is, IID data generation and non-IID data generation.\nprepare_train_test_even: partition the dataset into the local training data of clients, and a server side testing data.\ndef _train_test_split(local_data: Dataset) -&gt; DatasetDict:\n    \"\"\"\n    This function splits a given local dataset of a client into training and validation datasets.\n\n    Parameters\n    ----------\n    loca_data (Dataset): \n        The local dataset of a client to be split.\n\n    Returns\n    -------\n    data_dict (DatasetDict): \n        The training and validation datasets of the client.\n    \"\"\"\n    train_val_client_split = local_data.train_test_split(test_size=0.2, seed=RANDOM_SEEDS)  # 80% local training data, 20% local validation data\n    data_dict = DatasetDict({\n                'train': train_val_client_split['train'],\n                'validation': train_val_client_split['test'],\n                })\n    return data_dict\n\n\ndef prepare_train_test_iid(raw_datasets: Dataset,  num_clients: int) -&gt; Tuple[List[DatasetDict], Dataset]:\n    \"\"\"\n    Prepares the training and testing datasets for a federated learning scenario where the data is partitioned across \n    multiple clients in an IID (Independent and Identically Distributed) manner.\n\n    Parameters\n    ----------\n    raw_datasets: Dataset\n        The raw dataset containing the URLs and their corresponding labels.\n\n    Returns\n    -------\n    client_datasets: List[DatasetDict]\n        A list of datasets for each client, each containing the training and validation subsets.\n    server_test_dataset: Dataset\n        The dataset used by the central server for central evaluation.\n    \"\"\"\n    train_test_split = raw_datasets.train_test_split(test_size=0.2, seed=42)\n    client_dataset = train_test_split['train']\n    server_test_dataset = train_test_split['test']\n    partition_size = len(client_dataset) // num_clients # `num_clients` is the total number of clients in the federated learning process. `partition_size` is the number of records in each client's local data.\n\n    client_datasets = []\n    for _ in range(num_clients):\n        client_split = client_dataset.train_test_split(train_size=partition_size)\n        client_dataset = client_split['test'] # The remaining data will be divided among the other clients.\n        client_datasets.append(_train_test_split(client_split['train']))\n    return client_datasets, server_test_dataset\nprepare_train_test_noniid: generate label-based non-IID distributed local datasets for clients. Assume we are working on a multi-class classification problem. A client is only assigned records from two specified categories in the dataset. For example, consider that we are working on a malicious URLs detection problem, where there are four categories, namely, benign, malicious, phishing, and defacement URLs. The first client will only have records from benign and phishing categories, the second client will only have records from benign and malicous categories, and the third client will only have records from benign and defacement categories.\ndef prepare_train_test_noniid(raw_datasets: Dataset, num_clients: int) -&gt; Tuple[List[DatasetDict], Dataset]:\n    \"\"\"\n    Prepares the training and testing datasets for a federated learning scenario where the data is partitioned across \n    multiple clients in a non-IID (Non-Independent and Identically Distributed) manner.\n\n    Parameters\n    ----------\n    raw_datasets: Dataset\n        The raw dataset containing the URLs and their corresponding labels.\n    num_clients: int\n        The total number of clients in the federated learning process.\n\n    Returns\n    -------\n    client_datasets: List[DatasetDict]\n        A list of datasets for each client, each containing the training and validation subsets.\n    server_test_dataset: Dataset\n        The dataset used by the central server for central evaluation.\n    \"\"\"\n\n    train_test_split = raw_datasets.train_test_split(test_size=0.2, seed=42)\n    clients_dataset, server_test_dataset = train_test_split['train'], train_test_split['test']\n    # label_id 0: benign\n    # label_id 1: malicious\n    # label_id 2: phishing\n    # label_id 3: defacement\n    whole_benign = clients_dataset.filter(lambda x: x['labels'] == 0)\n    benign_size_per_client = len(whole_benign) // num_clients\n\n    client_datasets = []\n    # Class 0 is benign\n    for cid in range(num_clients):\n        abnormal_urls = clients_dataset.filter(lambda x: x['labels'] == (cid + 1)) \n        client_split = whole_benign.train_test_split(train_size=benign_size_per_client, seed=42)\n        local_benign, whole_benign = client_split['train'], client_split['test']\n\n        local_dataset = concatenate_datasets([local_benign, abnormal_urls])\n        client_datasets.append(_train_test_split(local_dataset))\n\n    return client_datasets, server_test_dataset\nFor this project, we constructed our classifier using the pre-trained BERT model. To prepare the input data for the model, we used the tokenize function of BERT to tokenize each URL into a sequence of tokens compatible with the BERT model. We iterate the datasets generated from the above functions, and tokenize the datasets. The tokenizer is also provided by the HuggingFace Transformers library. In particular, tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\ndef tokenize_function(record):\n    return tokenizer(record[\"url\"], truncation=True, padding=True)\n\nfor client_dataset in client_datasets:\n    tokenized_datasets = (client_dataset\n                        .map(tokenize_function, batched=True)\n                        .remove_columns(\"url\"))\n    \n    # The main purpose of DataCollatorWithPadding is to dynamically pad input sequences in a batch with the padding token to match the longest sequence in that batch. \n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n    \n    # Wrap the tokenized dataset as DataLoader\n    trainloader = DataLoader(\n        tokenized_datasets[\"train\"],\n        shuffle=True,\n        batch_size=BATCH_SIZE,\n        collate_fn=data_collator,\n    )\n    # Wrap test and validation dataset as DataLoader, ....\nAfter prepraing our datasets, next step is to define our model. For this project, we build our classifier based on the pre-trained BERT model, which can be easily implemented using the API provided by HuggingFace Transformers.\nThe created model instance net consists of a pre-trained distilled BERT model and a linear layer for sequence label prediction.\ndef init_model(num_labels: int, fine_tune: bool = True) -&gt; torch.nn.Module:\n    \"\"\"\n    Initialize a BERT based sequence classifier.\n\n    Parameters\n    ----------\n    num_labels:\n        The number of classes the model should predict.\n    fine_tune:\n        If we want to fine tune the parameters of the pre-trained BERT model.\n\n    Returns\n    -------\n    net:\n        A BERT-based sequence classifier model with the specified number of labels. \n    \"\"\"\n    net = AutoModelForSequenceClassification.from_pretrained(CHECKPOINT, num_labels=num_labels).to(DEVICE)\n\n    if fine_tune == False:\n        for name, param in net.named_parameters():\n            if name.startswith(\"bert\"): # choose whatever you like here\n                param.requires_grad = False\n\n    net.train()\n    return net\nWe are almost done with our implementation. The final step is to implement the training function to train our model.\ndef train(net: nn.Module, trainloader: DataLoader, epochs: int) -&gt; None:\n    \"\"\"\n    Train the given neural network for the specified number of epochs using the given data loader.\n\n    Parameters\n    ----------\n    net : nn.Module\n        The neural network to train.\n    trainloader : DataLoader\n        The data loader containing the training data.\n    epochs : int\n        The number of epochs to train for.\n\n    Returns\n    -------\n    None\n    \"\"\"\n\n    optimizer = AdamW(net.parameters(), lr=5e-5)\n\n    for _ in tqdm.tqdm(range(epochs), desc='epoch'):\n      net.train()\n      total_loss = 0\n      for batch in tqdm.tqdm(trainloader, desc='iterate data'):\n          batch = {k: v.to(DEVICE) for k, v in batch.items()}\n          outputs = net(**batch)\n          logits = outputs.get(\"logits\")\n          loss_fct = nn.CrossEntropyLoss(\n                        weight=torch.tensor([1.0, 10.0, 10.0, 10.0], \n                        device=DEVICE)\n                      )\n          labels = batch.get(\"labels\")\n          loss = loss_fct(logits.view(-1, NUM_LABELS), labels.view(-1))\n          loss.backward()\n          optimizer.step()\n          optimizer.zero_grad()\n          total_loss += loss.item()\n      \n    torch.cuda.empty_cache()"
  },
  {
    "objectID": "posts/fed_trans/index.html#execution",
    "href": "posts/fed_trans/index.html#execution",
    "title": "Federated Malicious URL Detection with Flower and Transformers",
    "section": "Execution",
    "text": "Execution\nNow that we have completed the implementation of our federated learning system, it’s time to put it to the test. Now that we have completed the implementation of our federated learning system, it’s time to put it to the test. Consider we aim to simulate a federated learning system with 10 clients on a single machine. Flower provides two ways to run the system: (1) Flower launches 10 instances of FlowerClient on different machines or servers; (2) Flower offers a way to simulate a federated learning system with multiple clients on a single machine by creating FlowerClient instances only when necessary for training or evaluation. This helps prevent memory exhaustion. To enable this function, we need to implement a client_fn function that creates a FlowerClient instance on demand.\ndef get_client_fn(client_dataloaders):\n    \"\"\"\n    Return the function to create a client\n\n    Parameters\n    ----------\n    client_dataloaders:\n        Dataloader of the training data of all clients\n    \"\"\"\n    def client_fn(cid):\n        \"\"\"\n        Create a client instance\n        \n        Parameters\n        -----------\n        cid:\n            The client ID\n        \"\"\"\n        return MalURLClient(cid, net, client_dataloaders[int(cid)]['train'], client_dataloaders[int(cid)]['validation'], testloader)\n    \n    return client_fn\nIn this project, we focus on the second option of simulation. To start the simulatoin, we only a simple call the build-in function start_simulation.\n# Start simulation\nfl.simulation.start_simulation(\n    client_fn=get_client_fn(client_dataloaders),\n    num_clients=NUM_CLIENTS,\n    config=fl.server.ServerConfig(num_rounds=5),\n    strategy=strategy, # Server side strategy discussed in Section Server\n    client_resources=client_resources,\n)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zicun Cong",
    "section": "",
    "text": "Hi there!\n\nI’m a Staff Software Engineer at Fortinet, in charge of developing Machine Learning models for cybersecurity threats detection and prevention. I am passionate about developing Data Mining and Machine Learning algorithms that can extract insights from very large scale log data, graphs and text corpus, such as Frequent Pattern Mining and Graph Neural Networks. Prior to Fortinet, I worked at Datastory as a software engineer, working on analyzing social network data.\n\n\nI received my Ph.D. in Computing Science from Simon Fraser University in 2022, advised by Professor Jian Pei. Prior to that, I obtained my Master degree in Computing Science from Simon Fraser University and Bachelor degree from the School of Software Engineering, Sun Yat-sen University.\n\n\nPlease visit here for my full CV.\n\n\n\nResearch Interest\nI am interested in Cloud Computing, Trustworhty Data Analytics, and Data Pricing. My current focuses include:\n\nInterpretation on deep neural networks and statistical hypothesis\nFairness on graph neural networks\nEfficient, scalable, and interpretable data pricing models\nComputation infrastructure for ML and MLOp\n\n\n\nPublications\n\nZicun Cong, Baoxu Shi, Shan Li, Jaewon Yang, Jian Pei, Qi He. “FairSample: Training Fair and Accurate Graph Neural Networks Efficiently.” Under review\nJay Xu, Jian Pei, Zicun Cong. “Finding Multidimensional Simpson’s Paradox.” SIGKDD Explorations  [paper]\nXuan Luo, Jian Pei, Zicun Cong, Cheng Xu. “On Shapley Value in Data Assemblage Under Independent Utility.” Proc. VLDB Endow. 15, 11 (2022), 2761–2773. [paper]\nZicun Cong, Xuan Luo, Jian Pei, Feida Zhu, Yong Zhang. “Data Pricing in Machine Learning Pipelines.” Knowledge and Information Systems (KAIS), 2022. [paper]\nZicun Cong, Lingyang Chu, Yu Yang, Jian Pei.”Comprehensible counterfactual explanation on Kolmogorov-Smirnov test.” Proc. VLDB Endow. 14, 9 (May 2021), 1583–1596.  [paper]\nZicun Cong, Lingyang Chu, Lanjun Wang, Xia Hu, and Jian Pei. “Exact and Consistent Interpretation of Piecewise Linear Models Hidden behind APIs: A Closed Form Solution.” In 2020 IEEE 36th International Conference on Data Engineering (ICDE), pp. 613-624. IEEE, 2020. [paper]\n\n\n\nTalk\n\nJian Pei, Feida Zhu, Zicun Cong, Xuan Luo, Huiwen Liu, Xin Mu.. “Data Pricing and Data Asset Governance in the AI Era.” In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining (KDD ’21). Association for Computing Machinery, New York, NY, USA, 4058–4059. [tutorial webpage]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog! This is Zicun Cong. I’m a staff data scientist at a cyber security company, where I spend my days using machine learning to protect businesses and individuals from digital threats. Prior to that I spent several years pursuasing my PhD in trustworthy machine learning with a specific focus on model interpretability and fairness. I have broad interest in end to end data science, including cloud computing, database, data mining and machine learning. Outside of work, I’m passionate about sharing my knowledge and insights with others who are interested in the intersection of data science and cyber security."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Zicun Cong's Homepage",
    "section": "",
    "text": "Building Efficient Universal Classifiers with Natural Language Inference\n\n\n\n\n\n\n\nnlp\n\n\npaper\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2024\n\n\nZicun Cong\n\n\n\n\n\n\n  \n\n\n\n\nFederated Malicious URL Detection with Flower and Transformers\n\n\n\n\n\n\n\nproject\n\n\nnlp\n\n\nfederated learning\n\n\n\n\n\n\n\n\n\n\n\nApr 21, 2023\n\n\nZicun Cong\n\n\n\n\n\n\nNo matching items"
  }
]