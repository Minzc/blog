[
  {
    "objectID": "posts/cyber/FewShotLearningofTTPsClassificationUsingLargeLanguageModels/index.html",
    "href": "posts/cyber/FewShotLearningofTTPsClassificationUsingLargeLanguageModels/index.html",
    "title": "Few-Shot Learning of TTPs Classification Using Large Language Models",
    "section": "",
    "text": "This paper proposes a method that combines ChatGPT data augmentation with Instruction Supervised Fine-Tuning of open large language models.\nReal-world TTPs are often embedded in a vast amount of heterogeneous unstructured text. Relying solely on manual identification requires significant human resources and effort. Automating the efficient classification of TTPs from unstructured text becomes a crucial task.\nProminent TTPs description frameworks include Stride, Cyber Kill Chain, and MITRE ATT&CK.\n\nThis method which exhibits a long-tail issue [9] results in a lack of categories for 108 techniques, with some having only descriptions and others having only one procedure example.\nTraditional data augmentation methods prove insufficient to meet the needs of preserving context semantic integrity and enhancing the diversity of training samples."
  },
  {
    "objectID": "posts/cyber/hackmentor/index.html",
    "href": "posts/cyber/hackmentor/index.html",
    "title": "HackMentor: Fine-Tuning Large Language Models for Cybersecurity",
    "section": "",
    "text": "TLDR:\nThe paper introduces a methodology for enhancing the performance of large language models (LLMs) in cybersecurity applications. The authors propose the fine-tuning of existing LLMs using specialized domain instructions and conversations to create models termed HackMentor. These models outperform traditional LLMs by 10-25% in tasks aligned with cybersecurity prompts, providing more concise, human-like responses and facilitating localized, private deployment for data security."
  },
  {
    "objectID": "posts/cyber/hackmentor/index.html#technical-in-depth-discussion",
    "href": "posts/cyber/hackmentor/index.html#technical-in-depth-discussion",
    "title": "HackMentor: Fine-Tuning Large Language Models for Cybersecurity",
    "section": "Technical In-depth Discussion:",
    "text": "Technical In-depth Discussion:\n\n\n\nThe methodology involves two main components: training data generation, LLMs fine-tuning, and model evaluation. Training data is meticulously compiled and constructed with cybersecurity-specific instructions and conversations. The fine-tuning employs LoRA, focusing on optimizing instruction following and conversational abilities by freezing the original model weights and training additional network layers. They perform fine-tuning on the Llama and Vicuna models of 7B and 13B using instructions and conversations (default parameters are used in fine-tuning), respectively.\nThe authors compiled and constructed instructional and conversational data specific to the cybersecurity domain. The guiding principles for this construction were richness, comprehensiveness, and security considerations.\n\nCybersecurity Instructions\nThey described instructional data as comprising instruction (task descriptions for LLMs), input (optional context for the instruction task), and output (answers generated by LLMs).\nThe creation process involves defining security instruction categories, creating seed instructions, and generating a comprehensive instructional dataset. They classified and organized task instructions related to cybersecurity, based on human expertise and GPT-4 categorization. For seed instructions, they constructed 144 seed instructions after meticulous curation to ensure quality, usability, and harmlessness, considering diverse aspects of cybersecurity knowledge. Last, they employ the self-instruct method and utilize the gpt-3.5-turbo model to generate a dataset comprising 14,000 instructions.\n\nDefinition of Cybersecurity Instruction Categories\nThe authors curated 8 major categories of domain-related instruction types, as displayed in Table I. Then, considering the variations in diversity, content completeness, and the distinct forms of instructions, inputs, and outputs within each category, they employed LLMs to generate 200 sub-instructions for each category. Through manual review and selection, they derived a final set of 97 sub-categories of security instruction types specific to the cybersecurity domain.\n\n\n\n\n\n\n\n\nInstruction Category\nDescription\nSubcategory Count\n\n\n\n\nConcept Explanation\nExplain various concepts in cybersecurity, such as DDoS attacks, encryption algorithms, etc.\n15\n\n\nCase Study\nProvide a cybersecurity case for the respondent to analyze the causes, impacts, and solutions.\n17\n\n\nTechnical Solution\nPresent a cybersecurity requirement or application scenario and ask for a technical implementation and solution.\n14\n\n\nPractical Exercise\nProvide a simulated network environment for the respondent to engage in attack and defense exercises.\n7\n\n\nSecurity Management\nPropose cybersecurity management requirements and ask the respondent to formulate management strategies or plans, including security training, awareness, etc.\n14\n\n\nPolicy and Regulations\nRequire explanations of laws and regulations related to network security, including privacy protection, data security, etc.\n10\n\n\nScenario Assessment\nProvide a virtual cybersecurity scenario for the respondent to make judgments and decisions.\n11\n\n\nIndustry Applications\nExplore the application of cybersecurity in a specific industry, such as finance, healthcare, retail, etc.\n9\n\n\n\n\n\nGeneration of Seed Instructions for Cybersecurity\n\nOpen QA Data: The process starts by gathering over 200 QA pairs from the Internet. From these, 15 pairs are carefully selected as seed instructions, considering the types of answers and the QA styles.\nGPT-4 Zero-shot Generation: Utilizing Alpaca’s prompt customization for the security domain, GPT-4 generates one hundred instances of instruction input and output (IIO) data based on tailored prompts. A meticulous manual screening process follows, focusing on instruction repetition and expression richness, to finalize 19 seed instructions.\nCybersecurity Instruction Categories: Based on major and sub-categories, LLMs are required to provide five question-answer pairs. These answers are manually scored on a scale of 1 to 5, with those scoring 5 selected as seed instructions, totaling 97 seed instructions.\nCybersecurity NLP Tasks: The paper categorizes cybersecurity NLP tasks into major categories like classification, named entity recognition, event detection, and regression. For each sub-category, one instruction is collected, resulting in a total of 13 seed instructions.\n\n\n\nSelf-instruct Method for Dataset Construction\nLeveraging the seed instructions, the self-instruct method is employed to construct the cybersecurity instruction dataset. This involves using the gpt-3.5-turbo model for generating instructions, ensuring diversity by retaining only those generated data with lower Rouge-L scores compared to existing instructions.\n\n\n\nCybersecurity Conversations\nAcknowledging the scarcity and difficulty in extracting standardized cybersecurity conversations, they incorporated an additional knowledge base specific to cybersecurity (such as Wikipedia) and used ChatGPT to generate relevant conversations.\nThe knowledge base included unstructured text such as threat intelligence and cybersecurity taxonomies. They adhered to principles like cold start, domain dependency, contextual relevance, and logical hierarchy to ensure data rationality in conversation generation.\nTo enhance the LLM’s understanding of prompts, they employed one-shot learning by providing an example to facilitate few-shot learning of ChatGPT, thereby generating the desired cybersecurity conversations."
  },
  {
    "objectID": "posts/cyber/hackmentor/index.html#experiment-discussion-and-summary",
    "href": "posts/cyber/hackmentor/index.html#experiment-discussion-and-summary",
    "title": "HackMentor: Fine-Tuning Large Language Models for Cybersecurity",
    "section": "Experiment Discussion and Summary:",
    "text": "Experiment Discussion and Summary:\nThe experimental evaluation uses metrics like WinRate, EloRating, and ZenoEval to compare HackMentor against traditional LLMs and benchmarks like ChatGPT. Results show that HackMentor significantly improves upon the capabilities of standard LLMs in cybersecurity contexts. The paper also discusses the challenges of training data scarcity and the balance between general and domain-specific performance​​."
  },
  {
    "objectID": "posts/building_efficient_universal_classifiers_with_natural_language_inference/index.html",
    "href": "posts/building_efficient_universal_classifiers_with_natural_language_inference/index.html",
    "title": "Building Efficient Universal Classifiers with Natural Language Inference",
    "section": "",
    "text": "Let’s face it: understanding and categorizing text can be a headache. Usually, you need to teach a computer model with lots of specific examples to get it right. But what if They could make this process easier and not so specific? That’s what this breakthrough method is all about. The paper aims to build a pre-trained language model to perform zero-shot text classification task. It uses a clever trick called Natural Language Inference (NLI) to teach models to classify text into categories without all the usual fuss."
  },
  {
    "objectID": "posts/building_efficient_universal_classifiers_with_natural_language_inference/index.html#building-a-universal-classifier-a-step-by-step-guide",
    "href": "posts/building_efficient_universal_classifiers_with_natural_language_inference/index.html#building-a-universal-classifier-a-step-by-step-guide",
    "title": "Building Efficient Universal Classifiers with Natural Language Inference",
    "section": "Building a Universal Classifier: A Step-by-Step Guide",
    "text": "Building a Universal Classifier: A Step-by-Step Guide\nGetting the Data Ready\nThey start by cleaning and organizing their data from different stheirces to make sure it’s all speaking the same language, so to speak. This step ensures everything is compatible with their model. Formulation of the input and output of the training data is shown in Figure 1.\nOptional Cleaning Up\nThere’s also a neat optional step where they clean the data further, removing any messy or confusing information. This helps their model learn better and faster, using a tool called CleanLab.\nCrafting Hypotheses\nHere’s where the NLI comes into play. They create “if this, then that” scenarios for each category. For example, if a text talks about cooking, it might belong to the “food” category. They also prepare for cases where the text doesn’t match, teaching the model to recognize when something doesn’t fit.\nTraining the Brain\nThey use a pre-trained model called DeBERTaV3 and teach it with their prepared data. Depending on the model’s size, this can take from 5 to 10 htheirs of computer time for one run, with a total of 6 to 15 days for all the training They need. Here are some of the hyperparameters They dial in for optimal performance: 1. Learning Rate: They adjust this based on the model size, setting it at 9e-6 for “large” models and 2e-5 for others. This determines how quickly the model learns from the data. 2. Gradient Accumulation Steps: This technique allows us to train with larger batches than their hardware could typically handle by spreading the computation across multiple steps. 3. Training Epochs: They run the training for 3 epochs, allowing the model to learn from the dataset three full times. 4. Warmup Ratio and Theyight Decay: These are tTheyaked to optimize training dynamics and prevent overfitting, where warmup ratio and Theyight decay are set to 0.06 and 0.01, respectively.\nOne training run on around 9000000 concatenated hypothesis-premise pairs for 3 epochs takes around 5 htheirs for DeBERTaV3-base and 10 htheirs for DeBERTaV3-large on one A100 40GB GPU.\nEvaluation\nAfter all that training, They check how well their model can classify new texts it hasn’t seen before. The model performance is shown in Figure 2. Overall, deberta-v3-zeroshot-v1.1-all-33 significantly outperforms the NLI-only model both on held-in and held-out tasks.\nThe authors have two insights. First, models trained with a mix of NLI data and non-NLI data achieve overall better zeroshot performance than the NLI-only model (+9.4% on average). Having seen different zeroshot-style hypotheses helps the model generalize to other unseen tasks and hypotheses (positive transfer). Second, there are a few cases of negative transfer. On a few datasets, the NLI-only model performs better than deberta-v3-zeroshot-v1.1-heldout, indicating that the additional task-mix can make the model over- or underpredict a few classes."
  },
  {
    "objectID": "posts/tabllm/index.html",
    "href": "posts/tabllm/index.html",
    "title": "TabLLM: Few-shot Classification of Tabular Data with Large Language Models",
    "section": "",
    "text": "TLDR\nThe study introduces a method that employs LLMs for zero-shot and few-shot classification of tabular data by serializing the data into natural-language strings. This approach not only simplifies the process but also demonstrates superior performance over traditional deep-learning and gradient-boosted tree methods in various benchmarks. Particularly, in the few-shot learning scenario, this method shows promising improvements in predictive performance.\n\n\nTechnical\nThe core of their methodology involves the conversion of tabular data into a format understandable by LLMs—natural language strings. This serialization process, coupled with a brief description of the classification problem, prepares the data for processing by LLMs without prior training specifically on tabular data. They explored several serialization techniques, including templates and table-to-text models, to identify the most effective method. Their experiments utilized the T0 language model and GPT-3, leveraging their inherent understanding of natural language to classify tabular datasets accurately.\nIn the few-shot setting, they fine-tuned the LLM using a subset of labeled examples through a parameter-efficient method known as T-Few. This approach allowed us to adapt the LLM’s parameters specifically for their classification tasks, improving accuracy with minimal data.\nWe first serialize the feature names and values into a natural language string. They evaluate different strategies. This string is then combined with a task-specific prompt. To get predictions, we obtain output probabilities from the LLM for each of a pre-specified set of verbalizer tokens (e.g., “Yes”, “No”), which map to class labels (e.g., 1, −1). If 𝑘 &gt; 0, they use the 𝑘 labeled examples to fine-tune the large language model using T-Few (Liu et al., 2022). Finally, they use the (possibly tuned) large language model to obtain predictions on unlabeled examples. The example prompt is shown in Figure 2.\n\n\n\n\n\n\n\n\nExperiment\nFigure 3. shows the average AUC and SD of different serializations across nine public datasets. The Text Template serialization performed very well across all experiments. In the zero-shot setting, the Text Template showed improvements over List Template, indicating the benefit of a serialization that is closer to the training distribution of T0. However, these differences al- ready vanished for 8 training examples. Hence, very few training examples might already suffice to adjust for dif- ferent templates. This suggests that sophisticated serializa- tions might be unnecessary when some training data exists. An interesting finding is that LLM for serialization performs worse than the simple text template based method, as the LLM can have hallucination, which introduces noise into the training data. Using only feature values had a poor performance for zero and very few shots, but the performance equalized with more training examples.\n\n\n\nFigure 4 shows the average AUC and SD of TabLLM versus all baseline models across nine public datasets. In all cases, TabLLM’s performance improved with a higher number of shots. In the zero-shot setting, TabLLM was on par with GPT-3 even though GPT-3 is a much larger model than T0 (175B vs. 11B parame- ters). TabPFN consistently outperformed the other baseline models across all numbers of training examples. TabPFN reached TabLLM’s performance with 4 to 256 (Income) training examples. LR was the second-best baseline of- ten beating the tree models, which might be due to our ex- tensive parameter tuning (see Sec. 4 in the Supplement). TabLLM outperformed or was on par with the tree ensem- ble baselines until 256 training examples for all datasets except Calhousing and Jungle. For fewer shots, it often outperformed them by a large margin. XGBoost performed relatively poorly for few shots, which was probably due to overfitting on the small training and validation sets (as de- scribed in the previous section, we do not use large valida- tion sets for hyperparameter tuning to ensure the results are truly few-shot)."
  },
  {
    "objectID": "posts/fed_trans/index.html",
    "href": "posts/fed_trans/index.html",
    "title": "Federated Malicious URL Detection with Flower and Transformers",
    "section": "",
    "text": "Cybersecurity is becoming increasingly important in today’s digital landscape, and malicious URLs are one of the most common ways for attackers to compromise user systems. Traditionally, to buil a malicious URL detection model, a large amount of user data needs to be collected and stored at a centarlized server, which poses a significant privacy risk, especially for agencies with highly sensitive information, like banks. Despite the fact that individual users can create their own URL classifiers using their own data, the performance of these classifiers is often unsatisfactory due to the limited amount of data available to each user. Federated learning is a technique that enables users to collaboratively create a classifier that utilizes their large datasets while also prevsering data privacy.\nIn this project, we demonstrate the use of federated learning and transformers for malicious URL detection using the popuarly used Flower and Hugging Face Transformers libraries."
  },
  {
    "objectID": "posts/fed_trans/index.html#client",
    "href": "posts/fed_trans/index.html#client",
    "title": "Federated Malicious URL Detection with Flower and Transformers",
    "section": "Client",
    "text": "Client\nThe Client interface serves as the primary means of communication between the central server and clients. To reduce computation and communication cost, only a subset of clients are selected for model training during each epoch (also known as a communication round). The central server sends the parameters of the global model and the training instructions to the selected clients over the network. The selected clients then perform model training and evaluation using their local data and send the updated model parameters back to the central server.\nclass MalURLClient(fl.client.NumPyClient):\n    def __init__(self, cid: str, \n                 net: torch.nn.Module, \n                 trainloader: DataLoader, \n                 valloader: DataLoader,\n                 epoch: int) -&gt; None:\n        \"\"\"\n        Initializes the class with the specified parameters.\n\n        Parameters\n        ----------\n        cid : str\n            A string representing the ID of the class.\n        net : torch.nn.Module\n            The neural network to use in the class.\n        trainloader : DataLoader\n            The data loader for the training set.\n        valloader : DataLoader\n            The data loader for the validation set.\n        epoch : int\n            The number of epochs to train for.\n\n        Returns\n        -------\n        None\n        \"\"\"\n        self.net = net\n        self.trainloader = trainloader\n        self.valloader = valloader\n        self.cid = cid\n        self.epoch = epoch  \n    # Other necessary functions, which will be introduced below\nImplementing Client interface typically involves defining the following methods (although set_parameters is optional): get_parameters, set_parameters, fit, and evaluate. Here, we have the implementation details of the four functions\nget_parameters: return the model weight as a list of NumPy ndarrays\ndef get_parameters(self, config: dict) -&gt; List[np.ndarray]:\n    \"\"\"\n    Returns a list of the parameters of the neural network in the class.\n\n    Parameters\n    ----------\n    config : dict\n        A dictionary containing configuration parameters.\n\n    Returns\n    -------\n    List[np.ndarray]\n        A list of numpy arrays containing the parameters of the neural network.\n    \"\"\"\n    return [val.cpu().numpy() for _, val in self.net.state_dict().items()]\nset_parameters: update the local model weights with the parameters received from the server\ndef set_parameters(net: nn.Module, parameters: List[torch.Tensor]) -&gt; nn.Module:\n    \"\"\"\n    Sets the parameters of a PyTorch neural network module to the specified tensors.\n\n    Parameters\n    ----------\n    net: nn.Module\n        The neural network module to set the parameters for.\n    parameters: List[torch.Tensor]\n        The list of tensors to set the parameters of the neural network module to.\n\n    Returns\n    -------\n    nn.Module\n        The neural network module with updated parameters.\n    \"\"\"\n    params_dict = zip(net.state_dict().keys(), parameters)\n    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n    net.load_state_dict(state_dict, strict=True)\n    return net\nfit: performs four operations 1. set the local model weights 2. train the local model 3. receive the updated local model weights\ndef fit(self, parameters, config):\n    \"\"\"\n    Parameters\n    ----------\n    parameters: \n        The model parameters received from the central server.\n    config: \n        Configuration parameters which allow the\n        server to influence training on the client. It can be used to communicate arbitrary values from the server to the client,\n        for example, to set the number of (local) training epochs.\n    Returns\n    -------\n    parameters: \n        The locally updated model parameters.\n    num_examples:\n        The number of examples used for training.\n    metrics:\n        A dictionary mapping arbitrary string keys to values of type\n        bool, bytes, float, int, or str. It can be used to communicate\n        arbitrary values back to the server.\n    \"\"\"\n    set_parameters(self.net, parameters)\n    print(\"Training Started...\")\n    # train_client function train the local model using the client' local dataset.\n    # train_client function is defined in the utility file\n    train(self.net, self.trainloader, epochs=self.epoch)\n    print(\"Training Finished.\")\n    return self.get_parameters(config), len(self.trainloader), {}\nevaluate: evaluate the global model on the client’s local dataset\ndef evaluate(self, parameters, config):\n    \"\"\"\n    Evaluate the provided parameters using the locally held dataset.\n    Parameters\n    ----------\n    parameters :\n        The current (global) model parameters.\n    config : \n        Same as the config in fit function.\n    Returns\n    -------\n    loss : \n        The evaluation loss of the model on the local dataset.\n    num_examples : \n        The number of examples used for evaluation.\n    metrics : \n        A dictionary mapping arbitrary string keys to values of\n        type bool, bytes, float, int, or str. It can be used to\n        communicate arbitrary values back to the server.\n    \"\"\"\n    self.net = set_parameters(self.net, parameters)\n    # test function is defined in the utility file.\n    valid_loss, valid_accuracy, valid_f1 = test(self.net, self.valloader)\n    metrics = {\n        \"valid_accuracy\": float(valid_accuracy), \n        \"valid_loss\": float(valid_loss),\n        'valid_f1': float(valid_f1),\n    }\n    return float(valid_loss), len(self.valloader), metrics"
  },
  {
    "objectID": "posts/fed_trans/index.html#server",
    "href": "posts/fed_trans/index.html#server",
    "title": "Federated Malicious URL Detection with Flower and Transformers",
    "section": "Server",
    "text": "Server\nNow that we have a way to instantiate clients, we need to create our server in order to aggregate the results. Using Flower, this can be done very easily by first choosing a strategy. A stratey is about how the central server update the global model using the model updates aggregated from the clients. In this project, we adopt the celebrated FedAvg, which will define the global weights as the average of all the clients’ weights at each round.\nIt is very important to decide how to evaluate the learned model. In federated learning, two types of evaluations are commonly used, namely, centarlize evaluation and federated evaluation. The two evaluation methods can be used at the same time.\nCentralized Evaluation (or server-side evaluation): it works similarly to evaluation in centralized machine learning. If there’s a server-side dataset available, we can use it to evaluate the newly aggregated model after each round of training.\nFederated evaluation (or client-side evaluation): the central server sends the newly aggregated model to a set of selected clients. Each client then evaluates the model on its local dataset and sends the evaluation metrics back to the central server. The central server aggregated the received the metrics, such as by taking the average, as the evaluation metric.\nFederated evaluation is more powerful than centralized evaluation because it allows for evaluation over a larger set of data, which often leads to more realistic evaluation results. However, federated evaluation is more complex and requires caution. Since in each epoch, a subset of clients are randomly selected for model evaluation, the evaluation clients can change over consecutive rounds of learning, leading to unstable evaluation results even if the model remains unchanged.\nWe can mitigate the issue by selecting all clients for evaluation for stable evaluation, however, this results in large communication and computation cost, especially there are hundreds of or thousands of clients participating in the federated learning.\nNext, let’s implement our server using the FedAvg strategy provided by Flower.\nevaluate: the centralized evaluation function, which will be called by the server after every epoch of federated learning.\ndef get_evaluate_fn(net: torch.nn.Module, testloader: DataLoader) -&gt; Callable[[int, Any, dict], Tuple[float, dict]]:\n    \"\"\"\n    Return an evaluation function for centralized evaluation.\n\n    Parameters\n    ----------\n    net : torch.nn.Module\n        The model to be evaluated.\n    testloader : DataLoader\n        The dataset loader.\n    Returns\n    -------\n    Callable[[int, Any, dict], Tuple[float, dict]]\n        A function that evaluates the model on the given test data and returns the evaluation loss and metrics.\n    \"\"\"\n    def evaluate(server_round: int, parameters: Any, config: dict) -&gt; Tuple[float, dict]:\n        \"\"\"\n        Evaluate the model on the given test data and return the evaluation loss and metrics.\n\n        Parameters\n        ----------\n        server_round : int\n            The current epoch of federated learning.\n        parameters : Any\n            The current (global) model parameters.\n        config : dict\n            Same as the config in fit.\n\n        Returns\n        -------\n        Tuple[float, dict]\n            A tuple containing the evaluation loss and a dictionary of evaluation metrics.\n        \"\"\"\n        set_parameters(net, parameters)  # 'net' is the global model. Update model with the latest parameters\n        loss, accuracy, f1 = test(net, testloader)\n        return loss, {\"accuracy\": accuracy, 'f1': f1}\n\n    return evaluate\nweighted_average: metric aggregation used by federated evaluation.\ndef weighted_average(metrics: List[Tuple[int, Metrics]]) -&gt; Metrics:\n    \"\"\"\n    Multiply accuracy of each client by number of examples used.\n    Aggregate and return custom metric (weighted average).\n\n    Parameters\n    ----------\n    metrics: List[Tuple[int, Metrics]]\n        The list of local evaluation metrics sent by clients.\n        metrics[idx] is the evaluation sent by the `idx` evaluation client.\n        metrics[idx][0] is the number of records of the corresponding client.\n        metrics[idx][1] is the evaluation metrics of the corresponding clients.\n\n    Returns\n    -------\n    weight_metrics: Metrics\n        The weighted average of the federated evaluation.\n    \"\"\"\n    weight_metrics = {}\n    for metric_name in metrics[0][1].keys():\n      for num_examples, m in metrics:\n        metric = [num_examples * m[metric_name] for num_examples, m in metrics]\n        examples = [num_examples for num_examples, _ in metrics]\n        weight_metrics[metric_name] = sum(metric) / sum(examples)\n    \n    return weight_metrics\nNow, we are ready to create a FedAvg stretegy using the above defined federated and centralized evaluation functions.\nstrategy = fl.server.strategy.FedAvg(\n    fraction_fit = 2/NUM_CLIENTS, # Sample 2 available clients for training in each epoch\n    evaluate_metrics_aggregation_fn = weighted_average, # Use weighted average function to aggregate the local evaluation metrics of clients. \n    fraction_evaluate = 2/NUM_CLIENTS, # Sample 2 available clients for model evaluation\n    evaluate_fn=get_evaluate_fn(net, testloader)  # Pass the evaluation function\n)"
  },
  {
    "objectID": "posts/fed_trans/index.html#utility-functions",
    "href": "posts/fed_trans/index.html#utility-functions",
    "title": "Federated Malicious URL Detection with Flower and Transformers",
    "section": "Utility Functions",
    "text": "Utility Functions\nGreat work! We’re halfway through our journey. In this section, we’ll implement some helper functions, including a data split function and model training and evaluation functions. Let’s get started!\nThe distributions of the local datasets held by clients can have a significant impact on the performance of the collaboratively trained model in federated learning. This is because federated learning relies on stochastic gradient descent (SGD), and the independent and identically distributed (IID) sampling of the training data is important to ensure that the stochastic gradient is an unbiased estimate of the full gradient (Zhao et al. 2018).\nTo investigate the impact of local data distribution on the performance of the global model, we conducted experiments using two different methods to generate clients’ local data, that is, IID data generation and non-IID data generation.\nprepare_train_test_even: partition the dataset into the local training data of clients, and a server side testing data.\ndef _train_test_split(local_data: Dataset) -&gt; DatasetDict:\n    \"\"\"\n    This function splits a given local dataset of a client into training and validation datasets.\n\n    Parameters\n    ----------\n    loca_data (Dataset): \n        The local dataset of a client to be split.\n\n    Returns\n    -------\n    data_dict (DatasetDict): \n        The training and validation datasets of the client.\n    \"\"\"\n    train_val_client_split = local_data.train_test_split(test_size=0.2, seed=RANDOM_SEEDS)  # 80% local training data, 20% local validation data\n    data_dict = DatasetDict({\n                'train': train_val_client_split['train'],\n                'validation': train_val_client_split['test'],\n                })\n    return data_dict\n\n\ndef prepare_train_test_iid(raw_datasets: Dataset,  num_clients: int) -&gt; Tuple[List[DatasetDict], Dataset]:\n    \"\"\"\n    Prepares the training and testing datasets for a federated learning scenario where the data is partitioned across \n    multiple clients in an IID (Independent and Identically Distributed) manner.\n\n    Parameters\n    ----------\n    raw_datasets: Dataset\n        The raw dataset containing the URLs and their corresponding labels.\n\n    Returns\n    -------\n    client_datasets: List[DatasetDict]\n        A list of datasets for each client, each containing the training and validation subsets.\n    server_test_dataset: Dataset\n        The dataset used by the central server for central evaluation.\n    \"\"\"\n    train_test_split = raw_datasets.train_test_split(test_size=0.2, seed=42)\n    client_dataset = train_test_split['train']\n    server_test_dataset = train_test_split['test']\n    partition_size = len(client_dataset) // num_clients # `num_clients` is the total number of clients in the federated learning process. `partition_size` is the number of records in each client's local data.\n\n    client_datasets = []\n    for _ in range(num_clients):\n        client_split = client_dataset.train_test_split(train_size=partition_size)\n        client_dataset = client_split['test'] # The remaining data will be divided among the other clients.\n        client_datasets.append(_train_test_split(client_split['train']))\n    return client_datasets, server_test_dataset\nprepare_train_test_noniid: generate label-based non-IID distributed local datasets for clients. Assume we are working on a multi-class classification problem. A client is only assigned records from two specified categories in the dataset. For example, consider that we are working on a malicious URLs detection problem, where there are four categories, namely, benign, malicious, phishing, and defacement URLs. The first client will only have records from benign and phishing categories, the second client will only have records from benign and malicous categories, and the third client will only have records from benign and defacement categories.\ndef prepare_train_test_noniid(raw_datasets: Dataset, num_clients: int) -&gt; Tuple[List[DatasetDict], Dataset]:\n    \"\"\"\n    Prepares the training and testing datasets for a federated learning scenario where the data is partitioned across \n    multiple clients in a non-IID (Non-Independent and Identically Distributed) manner.\n\n    Parameters\n    ----------\n    raw_datasets: Dataset\n        The raw dataset containing the URLs and their corresponding labels.\n    num_clients: int\n        The total number of clients in the federated learning process.\n\n    Returns\n    -------\n    client_datasets: List[DatasetDict]\n        A list of datasets for each client, each containing the training and validation subsets.\n    server_test_dataset: Dataset\n        The dataset used by the central server for central evaluation.\n    \"\"\"\n\n    train_test_split = raw_datasets.train_test_split(test_size=0.2, seed=42)\n    clients_dataset, server_test_dataset = train_test_split['train'], train_test_split['test']\n    # label_id 0: benign\n    # label_id 1: malicious\n    # label_id 2: phishing\n    # label_id 3: defacement\n    whole_benign = clients_dataset.filter(lambda x: x['labels'] == 0)\n    benign_size_per_client = len(whole_benign) // num_clients\n\n    client_datasets = []\n    # Class 0 is benign\n    for cid in range(num_clients):\n        abnormal_urls = clients_dataset.filter(lambda x: x['labels'] == (cid + 1)) \n        client_split = whole_benign.train_test_split(train_size=benign_size_per_client, seed=42)\n        local_benign, whole_benign = client_split['train'], client_split['test']\n\n        local_dataset = concatenate_datasets([local_benign, abnormal_urls])\n        client_datasets.append(_train_test_split(local_dataset))\n\n    return client_datasets, server_test_dataset\nFor this project, we constructed our classifier using the pre-trained BERT model. To prepare the input data for the model, we used the tokenize function of BERT to tokenize each URL into a sequence of tokens compatible with the BERT model. We iterate the datasets generated from the above functions, and tokenize the datasets. The tokenizer is also provided by the HuggingFace Transformers library. In particular, tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\ndef tokenize_function(record):\n    return tokenizer(record[\"url\"], truncation=True, padding=True)\n\nfor client_dataset in client_datasets:\n    tokenized_datasets = (client_dataset\n                        .map(tokenize_function, batched=True)\n                        .remove_columns(\"url\"))\n    \n    # The main purpose of DataCollatorWithPadding is to dynamically pad input sequences in a batch with the padding token to match the longest sequence in that batch. \n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n    \n    # Wrap the tokenized dataset as DataLoader\n    trainloader = DataLoader(\n        tokenized_datasets[\"train\"],\n        shuffle=True,\n        batch_size=BATCH_SIZE,\n        collate_fn=data_collator,\n    )\n    # Wrap test and validation dataset as DataLoader, ....\nAfter prepraing our datasets, next step is to define our model. For this project, we build our classifier based on the pre-trained BERT model, which can be easily implemented using the API provided by HuggingFace Transformers.\nThe created model instance net consists of a pre-trained distilled BERT model and a linear layer for sequence label prediction.\ndef init_model(num_labels: int, fine_tune: bool = True) -&gt; torch.nn.Module:\n    \"\"\"\n    Initialize a BERT based sequence classifier.\n\n    Parameters\n    ----------\n    num_labels:\n        The number of classes the model should predict.\n    fine_tune:\n        If we want to fine tune the parameters of the pre-trained BERT model.\n\n    Returns\n    -------\n    net:\n        A BERT-based sequence classifier model with the specified number of labels. \n    \"\"\"\n    net = AutoModelForSequenceClassification.from_pretrained(CHECKPOINT, num_labels=num_labels).to(DEVICE)\n\n    if fine_tune == False:\n        for name, param in net.named_parameters():\n            if name.startswith(\"bert\"): # choose whatever you like here\n                param.requires_grad = False\n\n    net.train()\n    return net\nWe are almost done with our implementation. The final step is to implement the training function to train our model.\ndef train(net: nn.Module, trainloader: DataLoader, epochs: int) -&gt; None:\n    \"\"\"\n    Train the given neural network for the specified number of epochs using the given data loader.\n\n    Parameters\n    ----------\n    net : nn.Module\n        The neural network to train.\n    trainloader : DataLoader\n        The data loader containing the training data.\n    epochs : int\n        The number of epochs to train for.\n\n    Returns\n    -------\n    None\n    \"\"\"\n\n    optimizer = AdamW(net.parameters(), lr=5e-5)\n\n    for _ in tqdm.tqdm(range(epochs), desc='epoch'):\n      net.train()\n      total_loss = 0\n      for batch in tqdm.tqdm(trainloader, desc='iterate data'):\n          batch = {k: v.to(DEVICE) for k, v in batch.items()}\n          outputs = net(**batch)\n          logits = outputs.get(\"logits\")\n          loss_fct = nn.CrossEntropyLoss(\n                        weight=torch.tensor([1.0, 10.0, 10.0, 10.0], \n                        device=DEVICE)\n                      )\n          labels = batch.get(\"labels\")\n          loss = loss_fct(logits.view(-1, NUM_LABELS), labels.view(-1))\n          loss.backward()\n          optimizer.step()\n          optimizer.zero_grad()\n          total_loss += loss.item()\n      \n    torch.cuda.empty_cache()"
  },
  {
    "objectID": "posts/fed_trans/index.html#execution",
    "href": "posts/fed_trans/index.html#execution",
    "title": "Federated Malicious URL Detection with Flower and Transformers",
    "section": "Execution",
    "text": "Execution\nNow that we have completed the implementation of our federated learning system, it’s time to put it to the test. Now that we have completed the implementation of our federated learning system, it’s time to put it to the test. Consider we aim to simulate a federated learning system with 10 clients on a single machine. Flower provides two ways to run the system: (1) Flower launches 10 instances of FlowerClient on different machines or servers; (2) Flower offers a way to simulate a federated learning system with multiple clients on a single machine by creating FlowerClient instances only when necessary for training or evaluation. This helps prevent memory exhaustion. To enable this function, we need to implement a client_fn function that creates a FlowerClient instance on demand.\ndef get_client_fn(client_dataloaders):\n    \"\"\"\n    Return the function to create a client\n\n    Parameters\n    ----------\n    client_dataloaders:\n        Dataloader of the training data of all clients\n    \"\"\"\n    def client_fn(cid):\n        \"\"\"\n        Create a client instance\n        \n        Parameters\n        -----------\n        cid:\n            The client ID\n        \"\"\"\n        return MalURLClient(cid, net, client_dataloaders[int(cid)]['train'], client_dataloaders[int(cid)]['validation'], testloader)\n    \n    return client_fn\nIn this project, we focus on the second option of simulation. To start the simulatoin, we only a simple call the build-in function start_simulation.\n# Start simulation\nfl.simulation.start_simulation(\n    client_fn=get_client_fn(client_dataloaders),\n    num_clients=NUM_CLIENTS,\n    config=fl.server.ServerConfig(num_rounds=5),\n    strategy=strategy, # Server side strategy discussed in Section Server\n    client_resources=client_resources,\n)"
  },
  {
    "objectID": "posts/log_summary/index.html",
    "href": "posts/log_summary/index.html",
    "title": "Summarizing Logs with Pre-Trained Language Models",
    "section": "",
    "text": "In the complex landscape of IT operations, where the sheer volume of data can overwhelm even the most seasoned system engineers, the advent of AIOps (Artificial Intelligence for IT Operations) offers a beacon of hope. AIOps systems, leveraging automated approaches such as anomaly detection and root cause analysis (RCA), promise a new era of efficiency in maintaining system health. Among these approaches, log summarization stands out as a crucial tool for distilling impactful information from vast log data, providing engineers with the insights needed to swiftly address and mitigate system failures.\nLog summarization, as the term suggests, aims to condense extensive log entries into actionable insights, focusing on identifying failures and their root causes. This process involves two primary tasks: summarizing common log events linked to specific failures and distinguishing non-normal log events that deviate from standard operation. These tasks are designed to spotlight critical information, aiding in quicker and more accurate system diagnostics."
  },
  {
    "objectID": "posts/log_summary/index.html#technical-implementation-with-bart-and-pegasus",
    "href": "posts/log_summary/index.html#technical-implementation-with-bart-and-pegasus",
    "title": "Summarizing Logs with Pre-Trained Language Models",
    "section": "Technical Implementation with BART and PEGASUS",
    "text": "Technical Implementation with BART and PEGASUS\nThe technical implementation of these summarization tasks leverages two prominent NLP models: BART (Bidirectional and Auto-Regressive Transformers) and PEGASUS (Pre-training with Extracted Gap-sentences for Abstractive SUmmarization Sequence-to-sequence models). Both models are adapted to address the unique challenges posed by log data.\nTokenizer Limitations: One of the first hurdles is adapting the tokenizer—a component that breaks down text into manageable pieces, or tokens, for the model to process. Log data often contain compound words, technical terms, and identifiers that are not commonly found in natural language texts. To preserve the integrity of such terms, careful consideration is given to how tokenizers handle unseen or compound words, opting against splitting these terms unnecessarily.\nSyntactic Separation: Unlike traditional texts punctuated with clear markers of sentence boundaries, log messages often lack these cues and are instead separated by line breaks. To teach the models to recognize these breaks as meaningful separators, we introduce special tokens (e.g., semicolons or newline tokens) during training, helping the models distinguish between separate log entries.\nHandling Input Size: Given the verbose nature of log data, managing input size becomes crucial. Both BART and PEGASUS have constraints on the maximum size of input they can process. To circumvent this, log data is segmented into smaller chunks that fit within these constraints. This segmentation ensures that models can process and summarize the data effectively, albeit with the challenge of maintaining the coherence and continuity of information across segments."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zicun Cong",
    "section": "",
    "text": "Hi there!\n\nI’m an AI/ML manager at Zscaler, leading the development of machine learning models for cybersecurity threats detection and prevention. Prior to Zscaler, I was a Staff Software Engineer at Fortinet working on applying ML techniques for IoC discovery. Prior to Fortinet, I worked at Datastory as a software engineer, working on analyzing social network data.\n\n\nI am passionate about developing Data Mining and Machine Learning algorithms that can extract insights from very large scale log data, graphs and text corpus, such as Frequent Pattern Mining, Graph Neural Networks, and LLM.\n\n\nI received my Ph.D. and MSc. in Computing Science from Simon Fraser University, advised by Professor Jian Pei. Prior to that, I obtained my Bachelor degree from the School of Software Engineering, Sun Yat-sen University supervised by Dr. Arber Xu.\n\n\nPlease visit here for my full CV.\n\n\n\nResearch Interest\nI am interested in Cloud Computing, Trustworhty Data Analytics, and Data Pricing. My current focuses include:\n\nApplying Large Language Model to facilitate log analysis\nInterpretation on deep neural networks and statistical hypothesis\nFairness on graph neural networks\nEfficient, scalable, and interpretable data pricing models\nComputation infrastructure for ML and MLOp\n\n\n\nPublications\n\nZicun Cong, Baoxu Shi, Shan Li, Jaewon Yang, Jian Pei, Qi He. “FairSample: Training Fair and Accurate Graph Neural Networks Efficiently.” TKDE 2023  [paper]\nJay Xu, Jian Pei, Zicun Cong. “Finding Multidimensional Simpson’s Paradox.” SIGKDD Explorations  [paper]\nXuan Luo, Jian Pei, Zicun Cong, Cheng Xu. “On Shapley Value in Data Assemblage Under Independent Utility.” Proc. VLDB Endow. 15, 11 (2022), 2761–2773. [paper]\nZicun Cong, Xuan Luo, Jian Pei, Feida Zhu, Yong Zhang. “Data Pricing in Machine Learning Pipelines.” Knowledge and Information Systems (KAIS), 2022. [paper]\nZicun Cong, Lingyang Chu, Yu Yang, Jian Pei.”Comprehensible counterfactual explanation on Kolmogorov-Smirnov test.” Proc. VLDB Endow. 14, 9 (May 2021), 1583–1596.  [paper]\nZicun Cong, Lingyang Chu, Lanjun Wang, Xia Hu, and Jian Pei. “Exact and Consistent Interpretation of Piecewise Linear Models Hidden behind APIs: A Closed Form Solution.” In 2020 IEEE 36th International Conference on Data Engineering (ICDE), pp. 613-624. IEEE, 2020. [paper]\n\n\n\nTalk\n\nJian Pei, Feida Zhu, Zicun Cong, Xuan Luo, Huiwen Liu, Xin Mu.. “Data Pricing and Data Asset Governance in the AI Era.” In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining (KDD ’21). Association for Computing Machinery, New York, NY, USA, 4058–4059. [tutorial webpage]\n\n\n\nThesis\n\nTowards Trustworthy Data Analytics: Algorithmic Tools for Interpretability and Fairness [PDF]\nMining Identification Rules for Classifying Mobile Application Traffic [PDF]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog! This is Zicun Cong. I’m a staff data scientist at a cyber security company, where I spend my days using machine learning to protect businesses and individuals from digital threats. Prior to that I spent several years pursuasing my PhD in trustworthy machine learning with a specific focus on model interpretability and fairness. I have broad interest in end to end data science, including cloud computing, database, data mining and machine learning. Outside of work, I’m passionate about sharing my knowledge and insights with others who are interested in the intersection of data science and cyber security."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Zicun Cong's Homepage",
    "section": "",
    "text": "HackMentor: Fine-Tuning Large Language Models for Cybersecurity\n\n\n\n\n\n\n\nnlp\n\n\npaper\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2024\n\n\nZicun Cong\n\n\n\n\n\n\n  \n\n\n\n\nTabLLM: Few-shot Classification of Tabular Data with Large Language Models\n\n\n\n\n\n\n\nnlp\n\n\npaper\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2024\n\n\nZicun Cong\n\n\n\n\n\n\n  \n\n\n\n\nFew-Shot Learning of TTPs Classification Using Large Language Models\n\n\n\n\n\n\n\nnlp\n\n\npaper\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2024\n\n\nZicun Cong\n\n\n\n\n\n\n  \n\n\n\n\nSummarizing Logs with Pre-Trained Language Models\n\n\n\n\n\n\n\nnlp\n\n\npaper\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2024\n\n\nZicun Cong\n\n\n\n\n\n\n  \n\n\n\n\nBuilding Efficient Universal Classifiers with Natural Language Inference\n\n\n\n\n\n\n\nnlp\n\n\npaper\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2024\n\n\nZicun Cong\n\n\n\n\n\n\n  \n\n\n\n\nFederated Malicious URL Detection with Flower and Transformers\n\n\n\n\n\n\n\nproject\n\n\nnlp\n\n\nfederated learning\n\n\n\n\n\n\n\n\n\n\n\nApr 21, 2023\n\n\nZicun Cong\n\n\n\n\n\n\nNo matching items"
  }
]